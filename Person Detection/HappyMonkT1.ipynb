{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIVffjz_7QyV"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUUo2Xx3W1mm"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"WzdcVZiD5CcK4yTLocIJ\")\n",
        "project = rf.workspace(\"iit-jammu-fjbaf\").project(\"human-yqjiq\")\n",
        "dataset = project.version(1).download(\"yolov8\")\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5Z3Bl__t40A"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Lvb16qu4tr"
      },
      "outputs": [],
      "source": [
        "%pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kP79PIgv2Ku"
      },
      "outputs": [],
      "source": [
        "# Run inference on an image with YOLOv8n\n",
        "!yolo predict model=yolov8n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgyJi3SOyhKE"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kNFPAyXcGCL"
      },
      "outputs": [],
      "source": [
        "def load_label(label_path):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            label_class, x1, y1, x2, y2 = line.split()\n",
        "            x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)\n",
        "            label_class = int(label_class)\n",
        "            bboxes.append([x1, y1, x2, y2, label_class])\n",
        "\n",
        "    return bboxes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgA23CoudJ5P"
      },
      "outputs": [],
      "source": [
        "!pip install albumentations\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSh3s_Opiv1v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def load_label(label_path):\n",
        "    bboxes = []\n",
        "    with open(label_path, 'r') as f:\n",
        "        for line in f:\n",
        "            label_class, x1, y1, x2, y2 = line.split()\n",
        "            x1, y1, x2, y2 = float(x1), float(y1), float(x2), float(y2)\n",
        "            label_class = int(label_class)\n",
        "            bboxes.append([x1, y1, x2, y2, label_class])\n",
        "\n",
        "    return bboxes\n",
        "\n",
        "def apply_augmentations(image_paths, label_paths, output_dir, output_dir_l):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for image_path, label_path in zip(image_paths, label_paths):\n",
        "        img = Image.open(image_path)\n",
        "        bboxes = load_label(label_path)\n",
        "\n",
        "        augs = A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.2),\n",
        "            A.Blur(blur_limit=(3, 7), p=0.1),\n",
        "            A.RandomCrop(height=360, width=360, p=0.4),\n",
        "            A.Sharpen(p=0.5)\n",
        "        ],\n",
        "        bbox_params=A.BboxParams(format=\"yolo\"))\n",
        "\n",
        "        img = np.array(img)\n",
        "        augmented = augs(image=img, bboxes=bboxes)\n",
        "        img, targets = augmented['image'], augmented['bboxes']\n",
        "\n",
        "        # Convert targets to tensor\n",
        "        targets = torch.as_tensor(targets, dtype=torch.float)\n",
        "\n",
        "        # Save the augmented image and targets\n",
        "        image_name = os.path.basename(image_path)\n",
        "        torch.save(img, os.path.join(output_dir, f\"augmented_{image_name}\"))\n",
        "        with open(os.path.join(output_dir_l, f\"augmented_{image_name}.txt\"), 'w') as f:\n",
        "            for bbox in targets:\n",
        "                x1, y1, x2, y2, label_class = bbox\n",
        "                f.write(f\"{label_class} {x1} {y1} {x2} {y2}\\n\")\n",
        "\n",
        "def merge_data(original_image_paths, augmented_image_paths, original_label_paths, augmented_label_paths, output_dir):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for orig_img_path, aug_img_path, orig_label_path, aug_label_path in zip(\n",
        "            original_image_paths, augmented_image_paths, original_label_paths, augmented_label_paths):\n",
        "\n",
        "        # Read original and augmented images\n",
        "        orig_img = cv2.imread(orig_img_path)\n",
        "        aug_img = cv2.imread(aug_img_path)\n",
        "\n",
        "        # Ensure images have the same dimensions\n",
        "        min_height = min(orig_img.shape[0], aug_img.shape[0])\n",
        "        min_width = min(orig_img.shape[1], aug_img.shape[1])\n",
        "\n",
        "        orig_img = orig_img[:min_height, :min_width, :]\n",
        "        aug_img = aug_img[:min_height, :min_width, :]\n",
        "\n",
        "        # Read original and augmented labels\n",
        "        with open(orig_label_path, 'r') as orig_label_file:\n",
        "            orig_labels = orig_label_file.readlines()\n",
        "\n",
        "        with open(aug_label_path, 'r') as aug_label_file:\n",
        "            aug_labels = aug_label_file.readlines()\n",
        "\n",
        "        # Combine original and augmented labels\n",
        "        combined_labels = orig_labels + aug_labels\n",
        "\n",
        "        # Save merged image\n",
        "        merged_img = np.concatenate((orig_img, aug_img), axis=1)\n",
        "        merged_img_path = os.path.join(output_dir, f\"merged_{os.path.basename(orig_img_path)}\")\n",
        "        cv2.imwrite(merged_img_path, merged_img)\n",
        "\n",
        "        # Save merged labels\n",
        "        merged_label_path = os.path.join(output_dir, f\"merged_{os.path.basename(orig_label_path)}\")\n",
        "        with open(merged_label_path, 'w') as merged_label_file:\n",
        "            merged_label_file.writelines(combined_labels)\n",
        "\n",
        "# Example usage:\n",
        "original_image_paths = [f'/content/Human-1/train/images/{filename}' for filename in os.listdir('/content/Human-1/train/images/')]\n",
        "original_label_paths = [f'/content/Human-1/train/labels/{filename}' for filename in os.listdir('/content/Human-1/train/labels/')]\n",
        "\n",
        "augmented_image_paths = [f'/content/temp/{filename}' for filename in os.listdir('/content/temp/') if filename.startswith('augmented_')]\n",
        "augmented_label_paths = [f'/content/temp/{filename}' for filename in os.listdir('/content/temp/') if filename.startswith('augmented_')]\n",
        "\n",
        "output_directory = '/content/merged/'\n",
        "\n",
        "# Apply augmentations\n",
        "apply_augmentations(original_image_paths, original_label_paths, '/content/temp/', '/content/temp/')\n",
        "\n",
        "# Merge original and augmented data\n",
        "merge_data(original_image_paths, augmented_image_paths, original_label_paths, augmented_label_paths, output_directory)\n",
        "\n",
        "# Example usage:\n",
        "image_paths = [f'/content/Human-1/train/images/{filename}' for filename in os.listdir('/content/Human-1/train/images/')]\n",
        "label_paths = [f'/content/Human-1/train/labels/{filename}' for filename in os.listdir('/content/Human-1/train/labels/')]\n",
        "output_directory = '/content/temp/images'\n",
        "output_directory_l = '/content/temp/labels'\n",
        "\n",
        "\n",
        "apply_augmentations(image_paths, label_paths, output_directory,output_directory_l)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDh9lomk4QS1"
      },
      "outputs": [],
      "source": [
        "!yolo task=detect mode=val model=/content/runs/detect/train4/weights/last.pt data=/content/Human-1/data.yaml conf = 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k_LJWHj7VcM",
        "outputId": "365c880a-51f6-47e7-baf7-8ab5ca478673"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UTF-8\n",
            "\n",
            "UTF-8\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "print()\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "print(locale.getpreferredencoding())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-bYigKx99K4"
      },
      "outputs": [],
      "source": [
        "model = YOLO('yolov8n.pt')\n",
        "# results = model.train(data='data.yaml', epochs=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0ea0qIbCvcC",
        "outputId": "a00a8092-9691-4499-ce35-cf579ade7281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'person',\n",
              " 1: 'bicycle',\n",
              " 2: 'car',\n",
              " 3: 'motorcycle',\n",
              " 4: 'airplane',\n",
              " 5: 'bus',\n",
              " 6: 'train',\n",
              " 7: 'truck',\n",
              " 8: 'boat',\n",
              " 9: 'traffic light',\n",
              " 10: 'fire hydrant',\n",
              " 11: 'stop sign',\n",
              " 12: 'parking meter',\n",
              " 13: 'bench',\n",
              " 14: 'bird',\n",
              " 15: 'cat',\n",
              " 16: 'dog',\n",
              " 17: 'horse',\n",
              " 18: 'sheep',\n",
              " 19: 'cow',\n",
              " 20: 'elephant',\n",
              " 21: 'bear',\n",
              " 22: 'zebra',\n",
              " 23: 'giraffe',\n",
              " 24: 'backpack',\n",
              " 25: 'umbrella',\n",
              " 26: 'handbag',\n",
              " 27: 'tie',\n",
              " 28: 'suitcase',\n",
              " 29: 'frisbee',\n",
              " 30: 'skis',\n",
              " 31: 'snowboard',\n",
              " 32: 'sports ball',\n",
              " 33: 'kite',\n",
              " 34: 'baseball bat',\n",
              " 35: 'baseball glove',\n",
              " 36: 'skateboard',\n",
              " 37: 'surfboard',\n",
              " 38: 'tennis racket',\n",
              " 39: 'bottle',\n",
              " 40: 'wine glass',\n",
              " 41: 'cup',\n",
              " 42: 'fork',\n",
              " 43: 'knife',\n",
              " 44: 'spoon',\n",
              " 45: 'bowl',\n",
              " 46: 'banana',\n",
              " 47: 'apple',\n",
              " 48: 'sandwich',\n",
              " 49: 'orange',\n",
              " 50: 'broccoli',\n",
              " 51: 'carrot',\n",
              " 52: 'hot dog',\n",
              " 53: 'pizza',\n",
              " 54: 'donut',\n",
              " 55: 'cake',\n",
              " 56: 'chair',\n",
              " 57: 'couch',\n",
              " 58: 'potted plant',\n",
              " 59: 'bed',\n",
              " 60: 'dining table',\n",
              " 61: 'toilet',\n",
              " 62: 'tv',\n",
              " 63: 'laptop',\n",
              " 64: 'mouse',\n",
              " 65: 'remote',\n",
              " 66: 'keyboard',\n",
              " 67: 'cell phone',\n",
              " 68: 'microwave',\n",
              " 69: 'oven',\n",
              " 70: 'toaster',\n",
              " 71: 'sink',\n",
              " 72: 'refrigerator',\n",
              " 73: 'book',\n",
              " 74: 'clock',\n",
              " 75: 'vase',\n",
              " 76: 'scissors',\n",
              " 77: 'teddy bear',\n",
              " 78: 'hair drier',\n",
              " 79: 'toothbrush'}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GrAh0xkc-QIF"
      },
      "outputs": [],
      "source": [
        "result = model.predict(source = \"/content/WIN_20231117_16_40_07_Pro.mp4\", save = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}